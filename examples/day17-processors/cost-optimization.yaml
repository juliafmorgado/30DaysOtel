# Cost Optimization Processors
# Demonstrates filtering and sampling to reduce observability costs
#
# NOTE: This example uses batch processor for educational purposes.
# OpenTelemetry is moving batching to exporters for better reliability.
# See: https://github.com/open-telemetry/opentelemetry-collector/issues/8122

receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

processors:
  # Aggressive filtering to reduce costs
  filter/cost-reduction:
    traces:
      span:
        # Remove successful health checks
        - 'name matches ".*health.*" and attributes["http.response.status_code"] < 400'
        
        # Remove very short spans (likely noise)
        - 'duration < 1000000'  # Less than 1ms
        
        # Remove internal service calls that are successful and fast
        - 'attributes["rpc.system"] == "grpc" and attributes["rpc.grpc.status_code"] == 0 and duration < 10000000'  # < 10ms
        
        # Sample high-volume endpoints (keep only 10%)
        - 'name == "GET /api/search" and attributes["http.response.status_code"] < 400 and TraceID() % 10 != 0'
        
        # Remove debug spans in production
        - 'attributes["span.kind"] == "internal" and attributes["debug"] == "true"'
    
    metrics:
      metric:
        # Remove test metrics
        - 'name matches "test.*"'
        - 'HasAttrKeyOnDatapoint("test.label")'
    
    logs:
      log_record:
        # Remove debug logs in production
        - 'severity_text == "DEBUG"'
        - 'body matches ".*debug.*"'
  
  # Add cost tracking attributes
  attributes/cost-tracking:
    actions:
      - key: cost.tier
        value: "standard"
        action: insert
      - key: cost.tier
        value: "premium"
        action: upsert
        include:
          match_type: strict
          services: ["payment-service", "user-service"]
  
  # Efficient batching for cost reduction (educational - use exporter batching instead)
  batch/cost-optimized:
    timeout: 2s  # Longer timeout = fewer network calls
    send_batch_size: 2048  # Larger batches = more efficient

exporters:
  logging:
    loglevel: info

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [filter/cost-reduction, attributes/cost-tracking, batch/cost-optimized]
      exporters: [logging]
    
    metrics:
      receivers: [otlp]
      processors: [filter/cost-reduction, batch/cost-optimized]
      exporters: [logging]
    
    logs:
      receivers: [otlp]
      processors: [filter/cost-reduction, batch/cost-optimized]
      exporters: [logging]